# 分布式与一致性

## 分布式理论

面临的问题：

* 通信异常
* 网络分区
* 节点故障

### CAP理论

当系统产生网络分区时，我们只能在满足可用性或是一致性中二选一。

### BASE理论

BASE是对CAP中一致性和可用性权衡的结果

* Basically Available：允许损失部分可用性
* Soft state：允许系统在多个不同节点的数据副本之间进行数据同步的过程中存在延迟。
* Eventually consistent：统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态

## 一致性模型

### 严格一致性（strict consistency）

也叫线性一致性。

> 用户在各进程上所看到的命令操作顺序与全局时钟顺序一致

### 顺序一致性（sequential consistency）

>  用户在各进程上所看到的命令操作顺序都一致且逻辑正确，但和全局时钟顺序不同

|  -   | T1     | T2     | T3     | T4     |
| :--: | ------ | ------ | ------ | ------ |
|  P1  | W(x,1) |        |        |        |
|  P2  |        | W(x,2) |        |        |
|  P3  |        |        | R(x,2) | R(x,1) |
|  P4  |        |        | R(x,2) | R(x,1) |

如上图，按照全局时钟P3,P4应该先R(x,1)在R(x,2)，但是实际不是，因此其不满足严格一致性。

但是P3，P4所看到的顺序是一致的，且先看到P2 W(x,2) 再看到P1 W(x,1) 并没有逻辑问题因此其满足顺序一致性

### 因果一致性

> 用户在各进程上所看到有因果关系的命令顺序都一直且逻辑正确，其余命令在各进程上看到的可以不同

|  -   | T1     |        | T2     | T3     | T4     | T5     |
| :--: | ------ | ------ | ------ | ------ | ------ | ------ |
|  P1  | W(x,1) |        | W(x,3) |        |        |        |
|  P2  |        | R(x,1) | W(x,2) |        |        |        |
|  P3  |        |        |        | R(x,1) | R(x,2) | R(x,3) |
|  P4  |        |        |        | R(x,1) | R(x,3) | R(x,2) |

如上图，其中因果表现在P2在R(x,1)之后才W(x,2)，因此要满足因果一致性必须各进程所看到的W(x,2)要在W(x,1)之后。

P3,P4 R(x,2)都在R(x,1)之后因此其满足因果一致性，但是P3和P4顺序不一致因此其不满足顺序一致性

### Writes Follow Reads

> 如果单个进程看到看到一个值W1后在写入W2，那么在该进程上一定是先看到W2在看到W1

可以看作因果一致性的单机版

### PRAM (pipelined random access memory）

> 用户在各进程上看到的所有写操作，与每个进程本身写操作的相对顺序一致，其余命令可以不同

|  -   | T1     |        | T2     | T3     | T4     | T5     |
| :--: | ------ | ------ | ------ | ------ | ------ | ------ |
|  P1  | W(x,1) |        | W(x,3) |        |        |        |
|  P2  |        | R(x,1) | W(x,2) |        |        |        |
|  P3  |        |        |        | R(x,2) | R(x,1) | R(x,3) |
|  P4  |        |        |        | R(x,1) | R(x,3) | R(x,2) |

如上图，因为P3的R(x,2)在R(x,2)之前因此不满足因果一致性。

P3P4 都满足R(x,3)在R(x,1)之后因此是PRAM

### Read Your Writes

> 在单个进程中，如果写入一个值w，那在着之后的读一定要能读到这个值而不是之前的值

这里的重点在于只保证在**单个进程中**，而不保证在多个进程中也是如此。因此其实现的核心在于两次都访问**同一台机器**避免同步延迟而读不到

### Monotonic Reads

> 在单个进程中的两次读操作，第二次读取的数据不能早于第一次读取的数据版本

### Monotonic Write

> 如果一个进程写了w1然后写了w2，那么其他进程看的顺序也必须是先w1再w2

## 分布式ID

### 号段模式

数据库字段有以下几个tag(业务标识)、maixId（当前最大）、step（步长）

每次请求获取时开始事务，执行maxId = maxId +step, SELECT。原子性的返回一段ID数据，然后放在内存中使用。

### 雪花算法

0 + 41位时间戳 +10位机器码 + 12位序列号。机器码代表提供分布式ID的服务，每台机器自身进行原子自增

## 分布式事务

### 2PC

数据库层面的分布式事务模型，多个事务操作同时成功或是同时失败。

分为两个角色:`参与者`和`协调者`

分为两阶段：`prepare`和`commit`

1. prepare：协调者询问参与者是否准备好提交，这里准备好指，是否已经在日志（以MySQL为例的话可以理解为redo log）记录了全部将要提交的内容。只差最后一句`Commit Record`没写
2. commint：如果协调者收到所有参与者的prepared消息，则**先**将自己的事务状态改为提交，然后像所有参与者发送commit状态，反之亦然。

缺点：

* 协调者单点问题
* 提交阶段出现问题无法补救，只能等节点恢复。节点恢复以后可以根据协调者状态判断是要提交还是回滚
* 性能问题（三次数据持久化，每次都要等最慢的结束）
* 一致性风险：提交阶段中途指令发送失败导致节点状态不一致

> FLP不可能原理：如果宕机最后不能恢复，不存在任何一种分布式协议可以正确地达成一致性结果

### 3PC

分为三阶段：`canCommit`,`preCommit`和`doCommit`

新增一轮canCommit，减小2pc prepare阶段中如果有节点失败，那些已经写完的节点又要全部回滚的重负载操作。通过canCommit，尽可能保证preCommit一定能执行。解决2pc性能问题

如果`doCommit`阶段失败，参与者没有收到消息默认会进行**提交操作**。解决2pc单点问题

缺点：

* 多一轮通信还增加一致性风险

**以上两者都是对于数据源而言，下面讲的是针对服务而言**

### 最大努力交付

1. 将事务操作进行排序
2. 进行第一个事务，将业务操作和事务消息在同一个事务中在本地提交
3. 定时轮询检查消息表中还未完成的事务进行远程调用
4. 如果成功则返回响应，更新本地消息表。
5. 如果失败则不断重试，或者人工介入。

本地消息表方案只要第一步成功了就没有回滚一说，后续一定要重试成功或者人工介入。因此使用此种方案一**定要实现服务幂等性**。

该方式**不具备隔离性**，无法避免类似超售的问题

也可通过MQ实现重试

### TCC

* try :完成业务可执行检查（保障一致性），预留资源（保障隔离性，更新数据表为冻结状态等）
* confirm: 提交业务（需要幂等）
* cancel : 释放try中的资源，释放如果失败进行最大努力交付

缺点：

* 业务侵入性较高
* 如果不是自己的资源无法做try操作

### SAGA事务

将一个大事务拆成一个个子事务，T1，T2，T3....

为每一个子事务设计对应的补偿动作，C1，C2，C3...

T和C都要满足幂等性，C要做最大努力交付

也有saga log用于系统崩溃后的恢复









